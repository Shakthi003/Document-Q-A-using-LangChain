# ğŸ“„ Document Q&A using LangChain & Streamlit

An interactive Streamlit web app that allows users to upload PDF documents, create semantic embeddings locally using Ollama (Gemma), and ask natural language questions grounded in the uploaded files.

The app leverages LangChain, Ollama (Gemma 2B), and ChromaDB for intelligent document retrieval and question-answering â€” all running locally without cloud dependencies.

ğŸš€ Features

ğŸ“š Upload and process multiple PDF documents

ğŸ” Automatic text chunking and embedding creation with Gemma via Ollama

ğŸ’¬ Ask questions and get contextual answers from your PDFs

ğŸ§  Powered by LangChain, Ollama, and Chroma

ğŸ’¾ Persistent local vector database for quick retrieval

ğŸŒ Clean and responsive Streamlit interface

ğŸ§© Tech Stack
Component	Technology
Frontend UI	Streamlit
AI Framework	LangChain
Embeddings	Ollama (Gemma 2B)
Vector Store	ChromaDB
PDF Loader	LangChain PyPDFLoader
Language	Python 3.11+
âš™ï¸ Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/Shakthi003/Document-Q-A-using-LangChain.git
cd Document-Q-A-using-LangChain

2ï¸âƒ£ Create a Virtual Environment
python -m venv .venv
.venv\Scripts\activate   # On Windows

3ï¸âƒ£ Install Requirements
pip install -r requirements.txt

4ï¸âƒ£ Ensure Ollama is Installed

Download and install Ollama from https://ollama.ai

Then pull the Gemma model:

ollama pull gemma:2b

5ï¸âƒ£ Run the Streamlit App
streamlit run app.py

ğŸ§  Usage

Upload one or more PDF files under â€œğŸ“„ Upload PDF(s)â€.

Click ğŸ› ï¸ Process documents and (re)build index to generate embeddings.

Enter your question in the text box â€” the app retrieves and answers from your documents.

ğŸ› ï¸ Example Questions

â€œWhat is the purpose of this report?â€

â€œSummarize the main findings.â€

â€œWho is mentioned in section 2?â€

ğŸš§ Future Improvements

Add support for DOCX, TXT, and HTML documents

Allow switching between multiple Ollama models (Mistral, Llama3, Gemma)

Integrate chat history and memory

Enable streamed token responses for faster interaction


2ï¸âƒ£ Create a Virtual Environment

python -m venv .venv
.venv\Scripts\activate   # On Windows


3ï¸âƒ£ Install Requirements

pip install -r requirements.txt


4ï¸âƒ£ Ensure Ollama is Installed

Download and install Ollama from https://ollama.ai

Then pull the Gemma model:

ollama pull gemma:2b


5ï¸âƒ£ Run the Streamlit App

streamlit run app.py


ğŸ§  Usage

Upload one or more PDF files under â€œğŸ“„ Upload PDF(s)â€.

Click ğŸ› ï¸ Process documents and (re)build index to generate embeddings.

Enter your question in the text box â€” the app retrieves and answers from your documents.

ğŸ› ï¸ Example Questions

â€œWhat is the purpose of this report?â€

â€œSummarize the main findings.â€

â€œWho is mentioned in section 2?â€


ğŸš§ Future Improvements

Add support for DOCX, TXT, and HTML documents

Allow switching between multiple Ollama models (Mistral, Llama3, Gemma)

Integrate chat history and memory

Enable streamed token responses for faster interaction

---
